
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Guided Real Image Dehazing using YCbCr Color Space</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://fanjunkai1.github.io/projectpage/DCL/video/video-demo.mp4">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://fanjunkai1.github.io/projectpage/DCL/"/>
    <meta property="og:title" content="Depth-Centric Dehazing and Depth-Estimation from Real-World Hazy Driving Video" />
    <meta property="og:description" content="Real driving-video dehazing poses a significant challenge due to the inherent difficulty in acquiring precisely aligned hazy/clear video pairs for effective model training, especially in dynamic driving scenarios with unpredictable weather conditions. 
	In this paper, we propose a pioneering approach that addresses this challenge through a non-aligned regularization strategy. Our core concept involves identifying clear frames that closely match hazy frames, serving as references to supervise a video dehazing network. Our approach comprises 
	two key components: reference matching and video dehazing. Firstly, we introduce a non-aligned reference frame matching module, leveraging an adaptive sliding window to match high-quality reference frames from clear videos. Video dehazing incorporates flow-guided cosine attention sampler and
	deformable cosine attention fusion modules to enhance spatial multi-frame alignment and fuse their improved information. To validate our approach, we collect a GoPro-Hazy dataset captured effortlessly with GoPro cameras in diverse rural and urban road environments. Extensive experiments 
	demonstrate the superiority of the proposed method over current state-of-the-art methods in the challenging task of real driving-video dehazing. />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Driving-Video Dehazing with Non-Aligned Regularization for Safety Assistance" />
    <meta name="twitter:description" content="Real driving-video dehazing poses a significant challenge due to the inherent difficulty in acquiring precisely aligned hazy/clear video pairs for effective model training, especially in dynamic driving scenarios with unpredictable weather conditions.
	In this paper, we propose a pioneering approach that addresses this challenge through a non-aligned regularization strategy. Our core concept involves identifying clear frames that closely match hazy frames, serving as references to supervise a video dehazing network. Our approach comprises
	two key components: reference matching and video dehazing. Firstly, we introduce a non-aligned reference frame matching module, leveraging an adaptive sliding window to match high-quality reference frames from clear videos. Video dehazing incorporates flow-guided cosine attention sampler and
	deformable cosine attention fusion modules to enhance spatial multi-frame alignment and fuse their improved information. To validate our approach, we collect a GoPro-Hazy dataset captured effortlessly with GoPro cameras in diverse rural and urban road environments. Extensive experiments
	demonstrate the superiority of the proposed method over current state-of-the-art methods in the challenging task of real driving-video dehazing. />

    <meta name="twitter:image" content="https://fanjunkai1.github.io/projectpage/DCL/video/video-demo.mp4" />


	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <br><b> Guided Real Image Dehazing using YCbCr Color Space </b></br>
                <small>
					AAAI 2025
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://github.com/fiwy0527/AAAI_25_SGDN">
                          Wenxuan Fang<sup>1</sup>
                        </a>
                    </li>
					<li>
                        <a href="https://fanjunkai1.github.io/">
                          Junkai Fan<sup>1</sup>
                        </a>
                    </li>
					<li>
                        <a href="">
                          Yu Zheng<sup>1</sup>
                        </a>
                    </li>
					<li>
                        <a href="https://wengjiangwei.github.io/">
                          Jiangwei Weng<sup>1</sup>
                        </a>
                    </li>
					<li>
                        <a href="https://tyshiwo.github.io/">
                          Ying Tai<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://sites.google.com/view/junlineu/">
                          Jun Li*<sup>1</sup>
                        </a>
                    </li>
					</br>
					<li>
						<sup>1</sup>School of Computer Science and Engineering, Nanjing University of Science and Technology <br>
						<sup>2</sup>Intelligence science and technology, Nanjing University
					</li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2405.09996">
                            <image src="img/SGDN_paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="video/video-demo.mp4" type="video/mp4">-->
<!--                            <image src="img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                        <li>
                            <a href="https://github.com/fiwy0527/AAAI_25_SGDN">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Dataset</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/fiwy0527/AAAI_25_SGDN">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4> 
                            </a>
                        </li> 
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--				<h3>-->
<!--                    Video Results-->
<!--                </h3>-->
<!--                <video id="v0" width="100%" autoplay loop muted controls>-->
<!--                  <source src="video/video-demo.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--						</div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
				Image dehazing, particularly with learning-based methods, has gained significant attention due to its importance in real-world applications.
                    However, relying solely on the RGB color space often fall short, frequently leaving residual haze.
                    This arises from two main issues: the difficulty in obtaining clear textural features from hazy RGB images and the complexity of acquiring real haze/clean image pairs outside controlled environments like smoke-filled scenes.
                    To address these issues, we first propose a novel Structure Guided Dehazing Network (SGDN) that leverages the superior structural properties of YCbCr features over RGB.
                    SGDN comprises two key modules: Bi-Color Guidance Bridge (BGB) and Color Enhancement Module (CEM).
                    BGB integrates a phase integration module and an interactive attention module, utilizing the rich texture features of the YCbCr space to guide the RGB space, thereby recovering clearer features in both frequency and spatial domains.
                    To maintain tonal consistency, CEM further enhances the color perception of RGB features by aggregating YCbCr channel information.
                    Furthermore, for effective supervised learning, we introduce the Real-World Well-Aligned Haze (RW^2AH) dataset, which includes a diverse range of scenes from various geographical regions and climate conditions.
                    Experimental results demonstrate that our method surpasses existing state-of-the-art methods across multiple real-world smoke/haze datasets.
                </p>
            </div>
        </div>
		
		
		<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motivation
                </h3>
               <div class="text-center">
                    <img src="img/motivation.png" width="450"/>
					<p class="text-center">
					<br>
					Visual comparison of different color spaces:
                        (a) RGB features suffer from degradation, resulting in unclear textures, whereas YCbCr features are less impacted by fog and reveal more distinct textures.
                        (b) RGB models (e.g., MB-Taylor) leave residual haze, while YCbCr models (e.g., AIPNet) struggle with color distortion.
                        Our approach effectively removes heavy fog while preserving color accuracy.
					</p>
                </div>
			</div>
		</div>	
		


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <div class="text-center">
                    <img src="img/framework.png" width="750"/>
					<p class="text-justify">
					<br>
					The overall pipeline of our SGDN.
                        It includes the proposed Bi-Color Guidance Bridge (BGB) and Color Enhancement Module (CEM).
                        BGB promotes RGB features to produce clearer textures through YCbCr color space in both frequency and spatial domain, while CEM significantly enhances the visual contrast of the images.
					</p>
                </div>
            </div>
        </div>


        <div class="row">

            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Real-World Well-Aligned Haze Dataset
                </h3>
                <div class="text-center">
                    <p class="text-justify">
					To enable effective supervised learning,
                        we collect a real-world haze dataset featuring multiple scenes and varying haze concentrations,
                        named the Real-World Well-Aligned Haze (RW^2AH) dataset, with a total of 1758 image pairs.
                        The RW^2AH dataset primarily records haze/clean images captured by stationary webcams from YouTube,
                        with scenes including landscapes, vegetation, buildings and mountains.
					</p>
                    <img src="img/distribution.png" width="400"/>
                    <img src="img/RW2HD.png" width="750"/>

                </div>
                <h3>
                    Results
                </h3>
                 <div class="text-center">
                    <img src="img/results.png" width="750"/>
					<p class="text-center">
					Comparing dehazing results on real-world smoke and haze datasets.
					</p>
                </div>
				<div class="text-center">
                    <img src="img/real.png" width="750"/>

                </div>
				<br>

				
				<div class="text-center">
                    <img src="img/rtts.png" width="750"/>
					<p class="text-center">
					More Quantitative results. We compare our framework with previous state-of-the-art methods on RTTS.
					</p>
                </div>


            </div>
        </div>
		
		
            
<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>Citation</h3>-->
<!--				<p>If you find our work useful in your research, please consider citing:</p>			-->
<!--				<a href=""><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="img/paper_thumbnail.png" width=160></a>-->
<!--		-->
<!--			<textarea id="bibtex" class="form-control" readonly>-->
<!--			-->
<!--                @artical{fan2024Depth,-->
<!--                title={Depth-Centric Dehazing and Depth-Estimation from Real-World Hazy Driving Video},-->
<!--                author={Fan, Junkai and Wang, Kun and Yan, Zhiqiang and Chen, Xiang and Gao Shangbin and Li, Jun and Yang, Jian},-->
<!--                booktitle={arxiv},-->
<!--                pages={26109&#45;&#45;26119},-->
<!--                year={2024}-->
<!--                }-->
<!--            </textarea>-->
<!--            </div>-->
<!--        </div>-->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
